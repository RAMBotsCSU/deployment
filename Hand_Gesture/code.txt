import cv2
import numpy as np
import logging
from pycoral.utils import edgetpu
from pycoral.adapters import common, classify
import time
from enum import Enum
from threading import Thread
from queue import Queue


class HandGesture(Enum):
    FIST = (0, "Fist", "Push down")
    ONE = (1, "One", "Walk forward")
    PALM = (2, "Palm", "Push up")
    PEACE_INVERTED = (3, "Peace Inverted", "Dance")
    STOP = (4, "Stop", "Stop walking")

    def __init__(self, gesture_id, gesture_name, action):
        self.gesture_id = gesture_id
        self.gesture_name = gesture_name
        self.action = action

    @staticmethod
    def get_gesture_by_id(gesture_id):
        for gesture in HandGesture:
            if gesture.gesture_id == gesture_id:
                return gesture
        return None


def load_model(model_path: str):
    """Load the TFLite model with Edge TPU support."""
    try:
        interpreter = edgetpu.make_interpreter(model_path)
        interpreter.allocate_tensors()
        logging.info("Model loaded successfully.")
        return interpreter
    except Exception as e:
        logging.error(f"Failed to load model: {e}")
        raise


def load_labels(labels_path: str) -> dict:
    """Load labels from a text file."""
    try:
        with open(labels_path, 'r') as f:
            return {int(line.split()[0]): line.strip().split(maxsplit=1)[1] for line in f.readlines()}
    except Exception as e:
        logging.error(f"Failed to load labels: {e}")
        raise


def setup_camera(camera_index: int = 0, width: int = 224, height: int = 224):
    """Initialize and configure the camera."""
    cap = cv2.VideoCapture(camera_index)
    if not cap.isOpened():
        raise ValueError("Error: Camera not found or could not be opened.")
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)
    cap.set(cv2.CAP_PROP_FPS, 30)
    return cap


def preprocess_frame(frame, input_shape):
    """Preprocess the frame to match the model's expected input (float32)."""
    resized_frame = cv2.resize(frame, input_shape)
    resized_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)
    resized_frame = resized_frame.astype(np.float32) / 255.0  # Normalize to [0, 1]
    input_tensor = np.expand_dims(resized_frame, axis=0)
    return input_tensor


def run_inference(interpreter, input_tensor):
    """Run inference on the input tensor."""
    input_details = interpreter.get_input_details()
    interpreter.set_tensor(input_details[0]['index'], input_tensor)
    
    start_time = time.time()
    interpreter.invoke()  # Run inference
    inference_time = time.time() - start_time
    
    logging.info(f"Inference time: {inference_time:.4f} seconds")

    output_details = interpreter.get_output_details()
    return classify.get_classes(interpreter, top_k=1)


def display_results(frame, classes, labels, fps, position=(10, 30)):
    """Display the inference results on the frame."""
    y_offset = position[1]
    for c in classes:
        label = labels.get(c.id, f"Class {c.id}")
        score = c.score
        gesture = HandGesture.get_gesture_by_id(c.id)
        if gesture:
            text = f"{gesture.gesture_name}: {gesture.action} ({score:.2f})"
        else:
            text = f"{label}: {score:.2f}"

        # Display the gesture information on the frame
        cv2.putText(frame, text, (position[0], y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        y_offset += 30

    # Display FPS
    cv2.putText(frame, f"FPS: {fps:.2f}", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
    return frame


def run_hand_gesture():
    """Main function to run hand gesture recognition."""
    logging.basicConfig(level=logging.INFO)

    MODEL_FILE = "hand_command_v2_edgetpu.tflite"
    LABELS_FILE = "labels.txt"

    # Load model and labels
    interpreter = load_model(MODEL_FILE)
    labels = load_labels(LABELS_FILE) if LABELS_FILE else {}
    input_shape = common.input_size(interpreter)

    # Set up the camera
    cap = setup_camera()
    logging.info("Camera setup complete. Press 'q' to quit.")

    frame_queue = Queue(maxsize=1)
    result_queue = Queue(maxsize=1)
    last_classes = []  # Store the last inference result
    fps = 0

    def capture_frames():
        """Capture frames from the camera."""
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            if not frame_queue.full():
                frame_queue.put(frame)

    def process_frames():
        """Process frames for inference."""
        while True:
            if not frame_queue.empty():
                frame = frame_queue.get()
                input_tensor = preprocess_frame(frame, input_shape)
                classes = run_inference(interpreter, input_tensor)
                if not result_queue.full():
                    result_queue.put((frame, classes))

    # Start threads
    capture_thread = Thread(target=capture_frames, daemon=True)
    process_thread = Thread(target=process_frames, daemon=True)
    capture_thread.start()
    process_thread.start()

    try:
        frame = None  # Initialize frame to ensure it's always defined
        while True:
            loop_start_time = time.time()

            # Retrieve the latest inference result
            if not result_queue.empty():
                frame, last_classes = result_queue.get()

            # Use the last known frame if available
            if frame is not None:
                frame = display_results(frame, last_classes, labels, fps)

            # Calculate FPS
            fps = 1.0 / (time.time() - loop_start_time)

            # Show frame
            cv2.imshow("Hand Gesture Recognition", frame if frame is not None else np.zeros((224, 224, 3), dtype=np.uint8))

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    finally:
        cap.release()
        cv2.destroyAllWindows()


if __name__ == "__main__":
    run_hand_gesture()
