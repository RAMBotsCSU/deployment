import cv2
import numpy as np
import logging
from pycoral.utils import edgetpu
from pycoral.adapters import common, classify
import time
from enum import Enum
from concurrent.futures import ThreadPoolExecutor


class HandGesture(Enum):
    FIST = ("Fist", "Push down")
    ONE = ("One", "Walk forward")
    PALM = ("Palm", "Push up")
    PEACE_INVERTED = ("Peace Inverted", "Dance")
    STOP = ("Stop", "Stop walking")

    def __init__(self, gesture_name, action):
        self.gesture_name = gesture_name
        self.action = action

    def describe(self):
        return f"{self.gesture_name}: {self.action}"


def load_model(model_path: str):
    """Load the TFLite model with Edge TPU support."""
    try:
        interpreter = edgetpu.make_interpreter(model_path)
        interpreter.allocate_tensors()
        logging.info("Model loaded successfully.")
        return interpreter
    except Exception as e:
        logging.error(f"Failed to load model: {e}")
        raise


def load_labels(labels_path: str) -> dict:
    """Load labels from a text file."""
    try:
        with open(labels_path, 'r') as f:
            return {int(line.split()[0]): line.strip().split(maxsplit=1)[1] for line in f.readlines()}
    except Exception as e:
        logging.error(f"Failed to load labels: {e}")
        raise


def setup_camera(camera_index: int = 0, width: int = 224, height: int = 224):
    """Initialize and configure the camera."""
    cap = cv2.VideoCapture(camera_index)
    if not cap.isOpened():
        raise ValueError("Error: Camera not found or could not be opened.")
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)
    cap.set(cv2.CAP_PROP_FPS, 15)
    return cap


def preprocess_frame(frame, input_shape, input_details):
    """Preprocess the frame to match the model's expected input."""
    resized_frame = cv2.resize(frame, input_shape)
    resized_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)

    # Normalize input to match model requirements
    resized_frame = resized_frame.astype(np.float32) / 255.0

    # Handle UINT8 quantized model input
    scale, zero_point = input_details[0]['quantization']
    if scale > 0:
        resized_frame = ((resized_frame / scale) + zero_point).astype(np.float32)

    # Expand dimensions for batch input
    input_tensor = np.expand_dims(resized_frame, axis=0)
    return input_tensor


def run_inference(interpreter, input_tensor, score_threshold=0.6):
    """Run inference on the input tensor."""
    input_details = interpreter.get_input_details()
    interpreter.set_tensor(input_details[0]['index'], input_tensor)
    interpreter.invoke()

    output_details = interpreter.get_output_details()
    output_data = interpreter.get_tensor(output_details[0]['index'])
    logging.debug(f"Raw model output: {output_data}")

    classes = classify.get_classes(interpreter, top_k=1, score_threshold=score_threshold)
    return classes


def display_results(frame: np.ndarray, classes: list, labels: dict, fps: float, position: tuple = (10, 30)) -> np.ndarray:
    """Display the inference results on the frame."""
    y_offset = position[1]
    for c in classes:
        label = labels.get(c.id, f"Class {c.id}")
        score = c.score
        try:
            gesture = HandGesture[c.id]
            text = f"{gesture.gesture_name}: {gesture.action} ({score:.2f})"
        except KeyError:
            text = f"{label}: {score:.2f}"

        # Display the gesture information on the frame
        cv2.putText(frame, text, (position[0], y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        y_offset += 30  # Move text down for the next line

    # Display FPS
    cv2.putText(frame, f"FPS: {fps:.2f}", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
    return frame


def run_hand_gesture():
    """Main function to run hand gesture recognition."""
    logging.basicConfig(level=logging.INFO)

    MODEL_FILE = "hand_command_v2.tflite"
    LABELS_FILE = "labels.txt"

    # Load model and labels
    interpreter = load_model(MODEL_FILE)
    labels = load_labels(LABELS_FILE) if LABELS_FILE else {}
    input_shape = common.input_size(interpreter)
    input_details = interpreter.get_input_details()

    # Set up the camera
    cap = setup_camera()
    logging.info("Camera setup complete. Press 'q' to quit.")

    executor = ThreadPoolExecutor(max_workers=2)

    def process_frame(frame):
        input_tensor = preprocess_frame(frame, input_shape, input_details)
        return run_inference(interpreter, input_tensor, score_threshold=0.6)

    try:
        while True:
            start_time = time.time()

            ret, frame = cap.read()
            if not ret:
                logging.error("Error: Unable to read from the camera.")
                break

            # Run inference asynchronously
            future = executor.submit(process_frame, frame)
            classes = future.result()

            if classes:
                for c in classes:
                    try:
                        gesture = HandGesture[c.id]
                        print(f"Detected: {gesture.gesture_name} - {gesture.action} with confidence {c.score:.2f}")
                    except KeyError:
                        print(f"Detected: {labels.get(c.id, 'Unknown')} with confidence {c.score:.2f}")
            else:
                print("No confident predictions.")

            end_time = time.time()
            fps = 1.0 / (end_time - start_time)
            frame = display_results(frame, classes, labels, fps)

            # Show frame
            cv2.imshow("Hand Gesture Recognition", frame)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    finally:
        cap.release()
        cv2.destroyAllWindows()


if __name__ == "__main__":
    run_hand_gesture()
