import cv2
import numpy as np
import logging
from pycoral.utils import edgetpu
from pycoral.adapters import common, classify
import time
from enum import Enum
from threading import Thread, Lock
from queue import Queue


class HandGesture(Enum):
    FIST = (0, "Fist", "Push down")
    ONE = (1, "One", "Walk forward")
    PALM = (2, "Palm", "Push up")
    PEACE_INVERTED = (3, "Peace Inverted", "Dance")
    STOP = (4, "Stop", "Stop walking")

    def __init__(self, gesture_id, gesture_name, action):
        self.gesture_id = gesture_id
        self.gesture_name = gesture_name
        self.action = action

    @staticmethod
    def get_gesture_by_id(gesture_id):
        for gesture in HandGesture:
            if gesture.gesture_id == gesture_id:
                return gesture
        return None


def load_model(model_path: str):
    """Load the TFLite model with Edge TPU support."""
    try:
        interpreter = edgetpu.make_interpreter(model_path)
        interpreter.allocate_tensors()
        logging.info("Model loaded successfully.")
        return interpreter
    except Exception as e:
        logging.error(f"Failed to load model: {e}")
        raise


def load_labels(labels_path: str) -> dict:
    """Load labels from a text file."""
    try:
        with open(labels_path, 'r') as f:
            return {int(line.split()[0]): line.strip().split(maxsplit=1)[1] for line in f.readlines()}
    except Exception as e:
        logging.error(f"Failed to load labels: {e}")
        raise


def setup_camera(camera_index: int = 0, width: int = 224, height: int = 224):
    """Initialize and configure the camera."""
    cap = cv2.VideoCapture(camera_index)
    if not cap.isOpened():
        raise ValueError("Error: Camera not found or could not be opened.")
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)
    cap.set(cv2.CAP_PROP_FPS, 30)
    return cap


def preprocess_frame(frame, input_shape):
    """Preprocess the frame to match the model's expected input."""
    resized_frame = cv2.resize(frame, input_shape)
    resized_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)
    resized_frame = resized_frame.astype(np.uint8)  # Keep as uint8 for TPU
    return np.expand_dims(resized_frame, axis=0)


def run_inference(interpreter, input_tensor):
    """Run inference on the input tensor."""
    input_details = interpreter.get_input_details()
    interpreter.set_tensor(input_details[0]['index'], input_tensor)
    interpreter.invoke()

    output_details = interpreter.get_output_details()
    output_data = interpreter.get_tensor(output_details[0]['index'])
    classes = classify.get_classes(interpreter, top_k=1)
    return classes


def display_results(frame, classes, labels, fps, position=(10, 30)):
    """Display the inference results on the frame."""
    y_offset = position[1]
    for c in classes:
        label = labels.get(c.id, f"Class {c.id}")
        score = c.score
        gesture = HandGesture.get_gesture_by_id(c.id)
        if gesture:
            text = f"{gesture.gesture_name}: {gesture.action} ({score:.2f})"
        else:
            text = f"{label}: {score:.2f}"

        # Display the gesture information on the frame
        cv2.putText(frame, text, (position[0], y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        y_offset += 30

    # Display FPS
    cv2.putText(frame, f"FPS: {fps:.2f}", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
    return frame


def run_hand_gesture():
    """Main function to run hand gesture recognition."""
    logging.basicConfig(level=logging.INFO)

    MODEL_FILE = "hand_command_v2.tflite"
    LABELS_FILE = "labels.txt"

    # Load model and labels
    interpreter = load_model(MODEL_FILE)
    labels = load_labels(LABELS_FILE) if LABELS_FILE else {}
    input_shape = common.input_size(interpreter)

    # Set up the camera
    cap = setup_camera()
    logging.info("Camera setup complete. Press 'q' to quit.")

    frame_queue = Queue(maxsize=2)
    result_queue = Queue(maxsize=2)
    lock = Lock()

    def capture_frames():
        """Capture frames from the camera."""
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            if not frame_queue.full():
                frame_queue.put(frame)

    def process_frames():
        """Process frames for inference."""
        while True:
            if not frame_queue.empty():
                frame = frame_queue.get()
                input_tensor = preprocess_frame(frame, input_shape)
                classes = run_inference(interpreter, input_tensor)
                result_queue.put((frame, classes))

    # Start threads
    capture_thread = Thread(target=capture_frames, daemon=True)
    process_thread = Thread(target=process_frames, daemon=True)
    capture_thread.start()
    process_thread.start()

    try:
        while True:
            start_time = time.time()

            if not result_queue.empty():
                frame, classes = result_queue.get()
                end_time = time.time()
                fps = 1.0 / (end_time - start_time)
                frame = display_results(frame, classes, labels, fps)

                # Show frame
                cv2.imshow("Hand Gesture Recognition", frame)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    finally:
        cap.release()
        cv2.destroyAllWindows()


if __name__ == "__main__":
    run_hand_gesture()
